{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, \n",
    "                     parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, \n",
    "                     parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3567 entries, 0 to 3566\n",
      "Data columns (total 3 columns):\n",
      "id            3567 non-null int64\n",
      "created_at    3567 non-null datetime64[ns]\n",
      "text          3567 non-null object\n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3567 entries, 0 to 3566\n",
      "Data columns (total 3 columns):\n",
      "id            3567 non-null int64\n",
      "created_at    3567 non-null datetime64[ns]\n",
      "text          3567 non-null object\n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>820251730407473153</td>\n",
       "      <td>2017-01-14 12:50:26</td>\n",
       "      <td>Congressman John Lewis should spend more time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820255947956383744</td>\n",
       "      <td>2017-01-14 13:07:12</td>\n",
       "      <td>mention crime infested) rather than falsely co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id          created_at  \\\n",
       "0  820251730407473153 2017-01-14 12:50:26   \n",
       "1  820255947956383744 2017-01-14 13:07:12   \n",
       "\n",
       "                                                text  \n",
       "0  Congressman John Lewis should spend more time ...  \n",
       "1  mention crime infested) rather than falsely co...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>1020287981020729344</td>\n",
       "      <td>2018-07-20 12:43:05</td>\n",
       "      <td>China, the European Union and others have been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>1020285014616002560</td>\n",
       "      <td>2018-07-20 12:31:18</td>\n",
       "      <td>My deepest sympathies to the families and frie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id          created_at  \\\n",
       "3565  1020287981020729344 2018-07-20 12:43:05   \n",
       "3566  1020285014616002560 2018-07-20 12:31:18   \n",
       "\n",
       "                                                   text  \n",
       "3565  China, the European Union and others have been...  \n",
       "3566  My deepest sympathies to the families and frie...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(tweets, regexp):\n",
    "        tweets.loc[:, \"text\"].replace(regexp, \" \", inplace = True)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_by_regex(df_1, re.compile(r\"http.?://[^\\s]+[\\s]?\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_urls'] = remove_by_regex(df_1, re.compile(r\"http.?://[^\\s]+[\\s]?\"))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>820251730407473153</td>\n",
       "      <td>2017-01-14 12:50:26</td>\n",
       "      <td>Congressman John Lewis should spend more time ...</td>\n",
       "      <td>Congressman John Lewis should spend more time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820255947956383744</td>\n",
       "      <td>2017-01-14 13:07:12</td>\n",
       "      <td>mention crime infested) rather than falsely co...</td>\n",
       "      <td>mention crime infested) rather than falsely co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>820257714362314753</td>\n",
       "      <td>2017-01-14 13:14:13</td>\n",
       "      <td>INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...</td>\n",
       "      <td>INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>820425770925338624</td>\n",
       "      <td>2017-01-15 00:22:01</td>\n",
       "      <td>Congressman John Lewis should finally focus on...</td>\n",
       "      <td>Congressman John Lewis should finally focus on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>820450166331346944</td>\n",
       "      <td>2017-01-15 01:58:57</td>\n",
       "      <td>Inauguration Day is turning out to be even big...</td>\n",
       "      <td>Inauguration Day is turning out to be even big...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id          created_at  \\\n",
       "0  820251730407473153 2017-01-14 12:50:26   \n",
       "1  820255947956383744 2017-01-14 13:07:12   \n",
       "2  820257714362314753 2017-01-14 13:14:13   \n",
       "3  820425770925338624 2017-01-15 00:22:01   \n",
       "4  820450166331346944 2017-01-15 01:58:57   \n",
       "\n",
       "                                                text  \\\n",
       "0  Congressman John Lewis should spend more time ...   \n",
       "1  mention crime infested) rather than falsely co...   \n",
       "2  INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...   \n",
       "3  Congressman John Lewis should finally focus on...   \n",
       "4  Inauguration Day is turning out to be even big...   \n",
       "\n",
       "                                        text_no_urls  \n",
       "0  Congressman John Lewis should spend more time ...  \n",
       "1  mention crime infested) rather than falsely co...  \n",
       "2  INTELLIGENCE INSIDERS NOW CLAIM THE TRUMP DOSS...  \n",
       "3  Congressman John Lewis should finally focus on...  \n",
       "4  Inauguration Day is turning out to be even big...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO ALL AMERICANS🇺🇸 https://t.co/D7Es6ie4fY'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO ALL AMERICANS🇺🇸  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_no_urls'][55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove usernames (mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_urls_names'] = remove_by_regex(df_1, re.compile(r\"@[^\\s]+[\\s]?\"))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO ALL AMERICANS🇺🇸 https://t.co/D7Es6ie4fY'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO ALL AMERICANS🇺🇸  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_no_urls_names'][55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_no_urls_names_nums'] = remove_by_regex(df_1, re.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inauguration Day is turning out to be even bigger than expected. January 20th, Washington D.C. Have fun!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inauguration Day is turning out to be even bigger than expected. January th, Washington D.C. Have fun!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_no_urls_names_nums'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation, special symbols and converts hashtags to \"normal\" words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove in map(lambda r: re.compile(re.escape(r)), \n",
    "                  [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\", \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                   \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\", \"!\", \"?\", \".\", \"'\", \"--\", \"---\", \"#\", \"...\"]\n",
    "                 ):\n",
    "    df_1.loc[:, \"text\"].replace(remove, \" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_cleaned'] = df_1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On my way! #Inauguration2017 https://t.co/hOuMbxGnpe'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On my way   Inauguration   '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_cleaned'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels = ['text_no_urls', 'text_no_urls_names', 'text_no_urls_names_nums'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'created_at', 'text', 'text_cleaned'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_stemming'] = df['text_cleaned'].apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @MoskowitzEva: .@BetsyDeVos has the talent, commitment, and leadership capacity to revitalize our public schools and deliver the promise…'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt ha the talent commit and leadership capac to revit our public school and deliv the promise…'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_stemming'][25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_train = df['text_cleaned'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stemmed_train = df['text_stemming'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english')\n",
    "vect_cleaned_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english')\n",
    "vect_stemmed_bow = CountVectorizer(ngram_range=(1, 1), stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_bow = vect_bow.fit_transform(df_train)\n",
    "trump_cleaned_bow = vect_cleaned_bow.fit_transform(df_cleaned_train)\n",
    "trump_stemmed_bow = vect_stemmed_bow.fit_transform(df_stemmed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3567, 8486), (3567, 6582), (3567, 4885))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_bow.shape, trump_cleaned_bow.shape, trump_stemmed_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from collections import Counter\n",
    "\n",
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english')\n",
    "vect_cleaned_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english')\n",
    "vect_stemmed_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english')\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "#summaries = \"\".join(df['text'])\n",
    "#ngrams_summaries = vect_tfidf.build_analyzer()(summaries)\n",
    "\n",
    "#Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tfidf = vect_tfidf.fit_transform(df_train)\n",
    "trump_cleaned_tfidf = vect_cleaned_tfidf.fit_transform(df_cleaned_train)\n",
    "trump_stemmed_tfidf = vect_stemmed_tfidf.fit_transform(df_stemmed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3567, 8486), (3567, 6582), (3567, 4885))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tfidf.shape, trump_cleaned_tfidf.shape, trump_stemmed_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentence\n",
    "test_sentence = 'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.'\n",
    "test = ['Come on and kill Kenny!', \n",
    "        'Make America great again!', \n",
    "        'Beer... Beeeeer... Beeeeeeeeer... WOO-HOO!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = vect_bow.transform(test)\n",
    "test_tfidf = vect_tfidf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 8486), (3, 8486))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bow.shape, test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = test.replace(re.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "test_cleaned = test_cleaned.replace(re.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "test_cleaned = test_cleaned.replace(re.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = pd.Series(test_sentence)\n",
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = test_sentence.replace(re.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = test_sentence.replace(re.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = test_sentence.replace(re.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove in map(lambda r: re.compile(re.escape(r)), \n",
    "                  [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\", \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                   \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\", \"!\", \"?\", \".\", \"'\", \"--\", \"---\", \"#\", \"...\"]\n",
    "                 ):\n",
    "    test.replace(remove, \" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove in map(lambda r: re.compile(re.escape(r)), \n",
    "                  [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\", \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                   \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\", \"!\", \"?\", \".\", \"'\", \"--\", \"---\", \"#\", \"...\"]\n",
    "                 ):\n",
    "    test_sentence.replace(remove, \" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To all the little girls watching   never doubt that you are valuable and powerful   deserving of every chance   opportunity in the world '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Come on and kill Kenny \n",
       "1                     Make America great again \n",
       "2    Beer    Beeeeer    Beeeeeeeeer    WOO HOO \n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to all the littl girl watch never doubt that you are valuabl and power deserv of everi chanc opportun in the world'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_stemmed = test_sentence.apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split()))\n",
    "test_sentence_stemmed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stemmed = test_cleaned.apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              come on and kill kenni\n",
       "1            make america great again\n",
       "2    beer beeeeer beeeeeeeeer woo hoo\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned_bow = vect_cleaned_bow.transform(test_cleaned)\n",
    "test_stemmed_bow = vect_stemmed_bow.transform(test_stemmed)\n",
    "test_cleaned_tfidf = vect_cleaned_tfidf.transform(test_cleaned)\n",
    "test_stemmed_tfidf = vect_stemmed_tfidf.transform(test_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 8486), (3, 6582), (3, 4885))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf.shape, test_cleaned_tfidf.shape, test_stemmed_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm_bow = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)\n",
    "ocsvm_cleaned_bow = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)\n",
    "ocsvm_stemmed_bow = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)\n",
    "\n",
    "ocsvm_tfidf = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)\n",
    "ocsvm_cleaned_tfidf = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)\n",
    "ocsvm_stemmed_tfidf = svm.OneClassSVM(nu = 0.5, kernel = 'rbf', gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_bow = [1 for i in range(trump_bow.shape[0])]\n",
    "y_true_cleaned_bow = [1 for i in range(trump_cleaned_bow.shape[0])]\n",
    "y_true_stemmed_bow = [1 for i in range(trump_stemmed_bow.shape[0])]\n",
    "y_true_tfidf = [1 for i in range(trump_tfidf.shape[0])]\n",
    "y_true_cleaned_tfidf = [1 for i in range(trump_cleaned_tfidf.shape[0])]\n",
    "y_true_stemmed_tfidf = [1 for i in range(trump_stemmed_tfidf.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_bow.fit(trump_bow, y = y_true_bow)\n",
    "prediction_bow = ocsvm_bow.predict(test_bow)\n",
    "prediction_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_tfidf.fit(trump_tfidf, y = y_true_tfidf)\n",
    "prediction_tfidf = ocsvm_tfidf.predict(test_tfidf)\n",
    "prediction_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_cleaned_bow.fit(trump_cleaned_bow, y = y_true_cleaned_bow)\n",
    "prediction_cleaned_bow = ocsvm_cleaned_bow.predict(test_cleaned_bow)\n",
    "prediction_cleaned_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_cleaned_tfidf.fit(trump_cleaned_tfidf, y = y_true_cleaned_tfidf)\n",
    "prediction_cleaned_tfidf = ocsvm_cleaned_tfidf.predict(test_cleaned_tfidf)\n",
    "prediction_cleaned_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_stemmed_bow.fit(trump_stemmed_bow, y = y_true_stemmed_bow)\n",
    "prediction_stemmed_bow = ocsvm_stemmed_bow.predict(test_stemmed_bow)\n",
    "prediction_stemmed_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocsvm_stemmed_tfidf.fit(trump_stemmed_tfidf, y = y_true_stemmed_tfidf)\n",
    "prediction_stemmed_tfidf = ocsvm_stemmed_tfidf.predict(test_stemmed_tfidf)\n",
    "prediction_stemmed_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TweetTokenizer, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tok = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.7 ms, sys: 3.81 ms, total: 50.5 ms\n",
      "Wall time: 49.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_tweets = []\n",
    "for tweet in df_train:\n",
    "    tokenized = regexp_tok.tokenize(tweet)\n",
    "    tokenized_tweets.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 s, sys: 20.6 ms, total: 1.94 s\n",
      "Wall time: 944 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = Word2Vec(tokenized_tweets, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tokenized = []\n",
    "for tweet in df_train:\n",
    "    tweet_tokenized = regexp_tok.tokenize(tweet)\n",
    "    df_train_tokenized.append(tweet_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 49.7 ms, total: 28.6 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wmd_list = []\n",
    "for sentence in test:\n",
    "    wmd_list_temp = []\n",
    "    for tweet in df_train_tokenized:\n",
    "        wmd = w2v_model.wv.wmdistance(tweet, regexp_tok.tokenize(sentence))\n",
    "        wmd_list_temp.append(wmd)\n",
    "    wmd_list.append(np.max(wmd_list_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.101997933602571 is a valid value\n",
      "6.198953959381104 is a valid value\n",
      "inf is NOT a valid value\n"
     ]
    }
   ],
   "source": [
    "for i in wmd_list:\n",
    "    if np.isinf(i) == False:\n",
    "        print('{} is a valid value'.format(i))\n",
    "    else:\n",
    "        print('{} is NOT a valid value'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity & Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Come on and kill Kenny \n",
       "1                     Make America great again \n",
       "2    Beer    Beeeeer    Beeeeeeeeer    WOO HOO \n",
       "dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Come on and kill Kenny \n",
       "1                     Make America great again \n",
       "2    Beer    Beeeeer    Beeeeeeeeer    WOO HOO \n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              come on and kill kenni\n",
       "1            make america great again\n",
       "2    beer beeeeer beeeeeeeeer woo hoo\n",
       "dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Come on and kill Kenny \n",
      "Cosine similarity Bag-Of-Words: 0.267\n",
      "Cosine similarity TF-IDF: 0.329\n",
      "Mean Cosine Similarity: 0.298\n",
      "\n",
      "OneClassSVM BOW prediction: 1\n",
      "OneClassSVM TF-IDF prediction: -1\n",
      "===========================================\n",
      "\n",
      ">>> Make America great again \n",
      "Cosine similarity Bag-Of-Words: 1.0\n",
      "Cosine similarity TF-IDF: 1.0\n",
      "Mean Cosine Similarity: 1.0\n",
      "\n",
      "OneClassSVM BOW prediction: 1\n",
      "OneClassSVM TF-IDF prediction: 1\n",
      "===========================================\n",
      "\n",
      ">>> Beer    Beeeeer    Beeeeeeeeer    WOO HOO \n",
      "Cosine similarity Bag-Of-Words: 0.0\n",
      "Cosine similarity TF-IDF: 0.0\n",
      "Mean Cosine Similarity: 0.0\n",
      "\n",
      "OneClassSVM BOW prediction: 1\n",
      "OneClassSVM TF-IDF prediction: 1\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for sentence in test:\n",
    "    sentence_bow = vect_bow.transform([sentence])\n",
    "    cos_dists_bow = cosine_similarity(trump_bow, sentence_bow)\n",
    "    \n",
    "    sentence_tfidf = vect_tfidf.transform([sentence])\n",
    "    cos_dists_tfidf = cosine_similarity(trump_tfidf, sentence_tfidf)\n",
    "    \n",
    "    mean_cos_dist = np.mean([np.max(cos_dists_bow), np.max(cos_dists_tfidf)])\n",
    "    \n",
    "    print('>>> {}'.format(sentence))\n",
    "    print('Cosine similarity Bag-Of-Words: {}'.format(round(np.max(cos_dists_bow), 3)))\n",
    "    print('Cosine similarity TF-IDF: {}'.format(round(np.max(cos_dists_tfidf), 3)))\n",
    "    print('Mean Cosine Similarity: {}\\n'.format(round(mean_cos_dist, 3)))\n",
    "    print('OneClassSVM BOW prediction: {}'.format(prediction_bow[i]))\n",
    "    print('OneClassSVM TF-IDF prediction: {}'.format(prediction_tfidf[i]))\n",
    "    print('===========================================\\n')\n",
    "    #print('Word2Vec Word Mover`s Distance: {}\\n'.format(wmd_list[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Come on and kill Kenny \n",
      "Cosine similarity Bag-Of-Words (cleaned): 0.267\n",
      "Cosine similarity TF-IDF (cleaned): 0.329\n",
      "Mean Cosine Similarity (cleaned): 0.298\n",
      "\n",
      "OneClassSVM BOW prediction (cleaned): 1\n",
      "OneClassSVM TF-IDF prediction (cleaned): -1\n",
      "===========================================\n",
      "\n",
      ">>> Make America great again \n",
      "Cosine similarity Bag-Of-Words (cleaned): 1.0\n",
      "Cosine similarity TF-IDF (cleaned): 1.0\n",
      "Mean Cosine Similarity (cleaned): 1.0\n",
      "\n",
      "OneClassSVM BOW prediction (cleaned): 1\n",
      "OneClassSVM TF-IDF prediction (cleaned): 1\n",
      "===========================================\n",
      "\n",
      ">>> Beer    Beeeeer    Beeeeeeeeer    WOO HOO \n",
      "Cosine similarity Bag-Of-Words (cleaned): 0.0\n",
      "Cosine similarity TF-IDF (cleaned): 0.0\n",
      "Mean Cosine Similarity (cleaned): 0.0\n",
      "\n",
      "OneClassSVM BOW prediction (cleaned): 1\n",
      "OneClassSVM TF-IDF prediction (cleaned): 1\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for sentence in test_cleaned:\n",
    "    sentence_cleaned_bow = vect_cleaned_bow.transform([sentence])\n",
    "    cos_dists_cleaned_bow = cosine_similarity(trump_cleaned_bow, sentence_cleaned_bow)\n",
    "    \n",
    "    sentence_cleaned_tfidf = vect_cleaned_tfidf.transform([sentence])\n",
    "    cos_dists_cleaned_tfidf = cosine_similarity(trump_cleaned_tfidf, sentence_cleaned_tfidf)\n",
    "\n",
    "    mean_cos_cleaned_dist = np.mean([np.max(cos_dists_cleaned_bow), np.max(cos_dists_cleaned_tfidf)])\n",
    "    \n",
    "    print('>>> {}'.format(sentence))\n",
    "    print('Cosine similarity Bag-Of-Words (cleaned): {}'.format(round(np.max(cos_dists_cleaned_bow), 3)))\n",
    "    print('Cosine similarity TF-IDF (cleaned): {}'.format(round(np.max(cos_dists_cleaned_tfidf), 3)))\n",
    "    print('Mean Cosine Similarity (cleaned): {}\\n'.format(round(mean_cos_cleaned_dist, 3)))\n",
    "    print('OneClassSVM BOW prediction (cleaned): {}'.format(prediction_cleaned_bow[i]))\n",
    "    print('OneClassSVM TF-IDF prediction (cleaned): {}'.format(prediction_cleaned_tfidf[i]))\n",
    "    print('===========================================\\n')\n",
    "    #print('Word2Vec Word Mover`s Distance: {}\\n'.format(wmd_list[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmed test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> come on and kill kenni\n",
      "Cosine similarity Bag-Of-Words (stemmed): 0.416\n",
      "Cosine similarity TF-IDF (stemmed): 0.409\n",
      "Mean Cosine Similarity (stemmed): 0.412\n",
      "\n",
      "OneClassSVM BOW prediction (stemmed): 1\n",
      "OneClassSVM TF-IDF prediction (stemmed): -1\n",
      "===========================================\n",
      "\n",
      ">>> make america great again\n",
      "Cosine similarity Bag-Of-Words (stemmed): 1.0\n",
      "Cosine similarity TF-IDF (stemmed): 1.0\n",
      "Mean Cosine Similarity (stemmed): 1.0\n",
      "\n",
      "OneClassSVM BOW prediction (stemmed): 1\n",
      "OneClassSVM TF-IDF prediction (stemmed): 1\n",
      "===========================================\n",
      "\n",
      ">>> beer beeeeer beeeeeeeeer woo hoo\n",
      "Cosine similarity Bag-Of-Words (stemmed): 0.0\n",
      "Cosine similarity TF-IDF (stemmed): 0.0\n",
      "Mean Cosine Similarity (stemmed): 0.0\n",
      "\n",
      "OneClassSVM BOW prediction (stemmed): 1\n",
      "OneClassSVM TF-IDF prediction (stemmed): 1\n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for sentence in test_stemmed:\n",
    "    sentence_stemmed_bow = vect_stemmed_bow.transform([sentence])\n",
    "    cos_dists_stemmed_bow = cosine_similarity(trump_stemmed_bow, sentence_stemmed_bow)\n",
    "    \n",
    "    sentence_stemmed_tfidf = vect_stemmed_tfidf.transform([sentence])\n",
    "    cos_dists_stemmed_tfidf = cosine_similarity(trump_stemmed_tfidf, sentence_stemmed_tfidf)\n",
    "\n",
    "    mean_cos_stemmed_dist = np.mean([np.max(cos_dists_stemmed_bow), np.max(cos_dists_stemmed_tfidf)])\n",
    "    \n",
    "    print('>>> {}'.format(sentence))\n",
    "    print('Cosine similarity Bag-Of-Words (stemmed): {}'.format(round(np.max(cos_dists_stemmed_bow), 3)))\n",
    "    print('Cosine similarity TF-IDF (stemmed): {}'.format(round(np.max(cos_dists_stemmed_tfidf), 3)))\n",
    "    print('Mean Cosine Similarity (stemmed): {}\\n'.format(round(mean_cos_stemmed_dist, 3)))\n",
    "    print('OneClassSVM BOW prediction (stemmed): {}'.format(prediction_stemmed_bow[i]))\n",
    "    print('OneClassSVM TF-IDF prediction (stemmed): {}'.format(prediction_stemmed_tfidf[i]))\n",
    "    print('===========================================\\n')\n",
    "    #print('Word2Vec Word Mover`s Distance: {}\\n'.format(wmd_list[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ocsvm_stemmed_tfidf.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#joblib.dump(df_train_tokenized, 'models/df_train_tokenized.pkl')\n",
    "\n",
    "#BOW Models\n",
    "joblib.dump(vect_bow, 'models/vect_bow.pkl')\n",
    "joblib.dump(vect_cleaned_bow, 'models/vect_cleaned_bow.pkl')\n",
    "joblib.dump(vect_stemmed_bow, 'models/vect_stemmed_bow.pkl')\n",
    "\n",
    "joblib.dump(trump_bow, 'models/trump_bow.pkl')\n",
    "joblib.dump(trump_cleaned_bow, 'models/trump_cleaned_bow.pkl')\n",
    "joblib.dump(trump_stemmed_bow, 'models/trump_stemmed_bow.pkl')\n",
    "\n",
    "#TF-IDF Models\n",
    "joblib.dump(vect_tfidf, 'models/vect_tfidf.pkl')\n",
    "joblib.dump(vect_cleaned_tfidf, 'models/vect_cleaned_tfidf.pkl')\n",
    "joblib.dump(vect_stemmed_tfidf, 'models/vect_stemmed_tfidf.pkl')\n",
    "\n",
    "joblib.dump(trump_tfidf, 'models/trump_tfidf.pkl')\n",
    "joblib.dump(trump_cleaned_tfidf, 'models/trump_cleaned_tfidf.pkl')\n",
    "joblib.dump(trump_stemmed_tfidf, 'models/trump_stemmed_tfidf.pkl')\n",
    "\n",
    "#OneClassSVM Model\n",
    "joblib.dump(ocsvm_bow, 'models/ocsvm_bow.pkl')\n",
    "joblib.dump(ocsvm_cleaned_bow, 'models/ocsvm_cleaned_bow.pkl')\n",
    "joblib.dump(ocsvm_stemmed_bow, 'models/ocsvm_stemmed_bow.pkl')\n",
    "joblib.dump(ocsvm_tfidf, 'models/ocsvm_tfidf.pkl')\n",
    "joblib.dump(ocsvm_cleaned_tfidf, 'models/ocsvm_cleaned_tfidf.pkl')\n",
    "joblib.dump(ocsvm_stemmed_tfidf, 'models/ocsvm_stemmed_tfidf.pkl')\n",
    "\n",
    "#joblib.dump(w2v_model, 'models/w2v.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
